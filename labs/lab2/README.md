# Lab 2: RÃ©vision de Code SÃ©curisÃ© avec LLM

**Auteurs:** Seynabou SOUGOU, Maxime XU  
**Classe:** ING 5 APP CYB - Groupe 2  
**Date:** ECE 2025/2026

---

## ğŸ“‹ Ce que nous avons fait

Ce lab nous a appris Ã  Ã©valuer et amÃ©liorer les prompts LLM pour la rÃ©vision de code sÃ©curisÃ©.

âœ… **SynthÃ¨se automatique de 30 cas de test**
- CrÃ©Ã© `generate_tests.py` pour synthÃ©tiser tests depuis les snippets de code
- Couvert: SQL Injection, DÃ©sÃ©rialisation, XSS, Path Traversal, Command Injection
- Tests en Python, JavaScript et Java

âœ… **Ã‰valuation comparative des prompts**
- **Baseline prompt:** RÃ©vision gÃ©nÃ©rale (76.7% de rÃ©ussite)
- **Secure prompt:** Consignes explicites de sÃ©curitÃ© (86.7% de rÃ©ussite)
- **AmÃ©lioration:** +10 points de pourcentage

âœ… **MÃ©triques & Rapports**
- GÃ©nÃ©rÃ© `metrics.csv` avec rÃ©sultats quantitatifs
- CrÃ©Ã© `brief.md` et `brief_fr.md` avec analyse dÃ©taillÃ©e
- Tous les tests passent: 1/1 âœ“

---

## ğŸ“ Fichiers ClÃ©s

**Tests synthÃ©tisÃ©s:**
- `_generated/tests_flat.yaml` - 30 cas de test gÃ©nÃ©rÃ©s automatiquement
- `generate_tests.py` - Script de synthÃ¨se

**Prompts testÃ©s:**
- `prompts/baseline_prompt.txt` - Prompt baseline
- `prompts/secure_review_prompt.txt` - Prompt sÃ©curisÃ©

**RÃ©sultats:**
- `reports/metrics.csv` - RÃ©sultats quantitatifs
- `reports/brief.md` - Analyse en franÃ§ais
- `reports/brief_fr.md` - Version alternative

---

## ğŸ› ï¸ PrÃ©requis

```bash
Python 3.11+
Node.js 22+ (pour promptfoo)
pip install -r ../../requirements.txt
npm install -g promptfoo
```

---

## ğŸš€ ExÃ©cuter les tests

```bash
# Du dossier lab2/
python -m unittest discover tests -v

# RÃ©sultat attendu: 1/1 test âœ“
```

---

## ğŸ“Š RÃ©sultats de l'Ã©valuation

| MÃ©trique | Baseline | Secure | AmÃ©lioration |
|----------|----------|--------|-------------|
| **Accuracy** | 76.7% | 86.7% | +10.0pp |
| **Tests passÃ©s** | 23/30 | 26/30 | +3 tests |
| **Faux positifs** | 2 | 1 | -1 |
| **Faux nÃ©gatifs** | 5 | 3 | -2 |

---

## ğŸ” Failles de sÃ©curitÃ© couvertes

1. **SQL Injection** - Injection SQL via paramÃ¨tres
2. **DÃ©sÃ©rialisation** - Deserialization attacks
3. **XSS** - Cross-Site Scripting
4. **Path Traversal** - Directory traversal attacks
5. **Command Injection** - OS command injection

---

## ğŸ“ Concepts appliquÃ©s

- **Prompt Engineering** - Optimisation des instructions LLM
- **Test Synthesis** - GÃ©nÃ©ration automatique de cas de test
- **Security Review** - Ã‰valuation de code sÃ©curisÃ©
- **Metrics Evaluation** - Mesure comparative

---

## ğŸ“ Notes importantes

1. **Authentique:** Tests synthÃ©tisÃ©s depuis code rÃ©el
2. **Comparable:** MÃªme baseline et prompts sÃ©curisÃ©s testÃ©s
3. **AmÃ©liorÃ©:** +10pp d'amÃ©lioration via better prompting
4. **DocumentÃ©:** Chaque test case tracÃ©

**Conclusion:** Lab 2 montre comment des prompts plus prÃ©cis amÃ©liorent significativement la qualitÃ© de la rÃ©vision de code LLM.
